Experiment dir : LM-TFM-wt103/20240202-052447
wandb: Currently logged in as: jincan333. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/caj20001/.netrc
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.11
wandb: Run data is saved locally in /home/caj20001/LoT/wandb/run-20240202_052448-1dn05eke
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Transformer_PTB_LoT_max_epoch25_alpha0.5_N5_T1.5_lr0.001_gpu2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jincan333/Transformer_LoT
wandb: üöÄ View run at https://wandb.ai/jincan333/Transformer_LoT/runs/1dn05eke
Loading cached dataset...
====================================================================================================
    - data : data/ptb/
    - dataset : wt103
    - n_layer : 4
    - n_head : 12
    - d_head : 41
    - d_embed : 410
    - d_model : 410
    - d_inner : 2100
    - dropout : 0.25
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.001
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.35
    - clip_nonemb : False
    - max_step : 5550
    - batch_size : 60
    - batch_chunk : 1
    - tgt_len : 70
    - eval_tgt_len : 70
    - ext_len : 0
    - mem_len : 70
    - not_tied : False
    - seed : 1
    - cuda : True
    - gpu : 2
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : False
    - log_interval : 200
    - eval_interval : 1000
    - work_dir : LM-TFM-wt103/20240202-052447
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : -1
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - alpha : 0.5
    - student_steps_ratio : 5
    - T : 1.5
    - exp_name : Transformer_PTB_LoT_max_epoch25_alpha0.5_N5_T1.5_lr0.001_gpu2
    - max_epoch : 25
    - start_epoch : -1
    - auto_step : 1
    - tied : True
    - n_token : 10000
    - n_all_param : 30099968
    - n_nonemb_param : 21878000
====================================================================================================
#params = 30099968
#non emb params = 21878000
/home/caj20001/miniconda3/envs/LoT/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
| epoch   1 step      200 |    200 batches | lr 0.000997 | ms/batch 463.25 | teacher_loss  6.05 | student_loss  5.17 | teacher ppl   426.134 | student ppl 175.97712
| epoch   2 step      400 |    178 batches | lr 0.000987 | ms/batch 468.88 | teacher_loss  5.24 | student_loss  4.50 | teacher ppl   189.494 | student ppl  89.90821
| epoch   3 step      600 |    156 batches | lr 0.000971 | ms/batch 470.88 | teacher_loss  4.98 | student_loss  4.28 | teacher ppl   145.693 | student ppl  72.53680
| epoch   4 step      800 |    134 batches | lr 0.00095 | ms/batch 470.56 | teacher_loss  4.82 | student_loss  4.14 | teacher ppl   124.263 | student ppl  62.81760
| epoch   5 step     1000 |    112 batches | lr 0.000922 | ms/batch 470.68 | teacher_loss  4.70 | student_loss  4.03 | teacher ppl   109.716 | student ppl  56.02951
----------------------------------------------------------------------------------------------------
| Eval   1 at step     1000 | time: 469.76s | teacher valid loss  4.49 | student valid loss  4.28 | teacher valid ppl    89.018 | student valid ppl    72.224
----------------------------------------------------------------------------------------------------
| epoch   6 step     1200 |     90 batches | lr 0.000889 | ms/batch 479.01 | teacher_loss  4.61 | student_loss  3.95 | teacher ppl   100.110 | student ppl  52.16866
| epoch   7 step     1400 |     68 batches | lr 0.000851 | ms/batch 472.64 | teacher_loss  4.52 | student_loss  3.88 | teacher ppl    91.837 | student ppl  48.25178
| epoch   8 step     1600 |     46 batches | lr 0.000809 | ms/batch 473.03 | teacher_loss  4.46 | student_loss  3.81 | teacher ppl    86.903 | student ppl  45.32909
| epoch   9 step     1800 |     24 batches | lr 0.000762 | ms/batch 472.74 | teacher_loss  4.40 | student_loss  3.75 | teacher ppl    81.473 | student ppl  42.71443
| epoch  10 step     2000 |      2 batches | lr 0.000712 | ms/batch 472.67 | teacher_loss  4.34 | student_loss  3.71 | teacher ppl    77.053 | student ppl  40.67602
----------------------------------------------------------------------------------------------------
| Eval   2 at step     2000 | time: 473.71s | teacher valid loss  4.34 | student valid loss  4.30 | teacher valid ppl    76.593 | student valid ppl    73.603
----------------------------------------------------------------------------------------------------
| epoch  10 step     2200 |    202 batches | lr 0.00066 | ms/batch 480.46 | teacher_loss  4.29 | student_loss  3.64 | teacher ppl    73.114 | student ppl  38.06286
| epoch  11 step     2400 |    180 batches | lr 0.000605 | ms/batch 472.84 | teacher_loss  4.25 | student_loss  3.60 | teacher ppl    70.116 | student ppl  36.50237
| epoch  12 step     2600 |    158 batches | lr 0.000549 | ms/batch 473.14 | teacher_loss  4.21 | student_loss  3.56 | teacher ppl    67.230 | student ppl  35.02950
| epoch  13 step     2800 |    136 batches | lr 0.000493 | ms/batch 472.73 | teacher_loss  4.17 | student_loss  3.51 | teacher ppl    64.634 | student ppl  33.42167
| epoch  14 step     3000 |    114 batches | lr 0.000436 | ms/batch 472.28 | teacher_loss  4.12 | student_loss  3.45 | teacher ppl    61.354 | student ppl  31.53923
----------------------------------------------------------------------------------------------------
| Eval   3 at step     3000 | time: 473.83s | teacher valid loss  4.28 | student valid loss  4.30 | teacher valid ppl    72.430 | student valid ppl    74.011
----------------------------------------------------------------------------------------------------
| epoch  15 step     3200 |     92 batches | lr 0.000381 | ms/batch 479.32 | teacher_loss  4.08 | student_loss  3.41 | teacher ppl    59.024 | student ppl  30.41148
| epoch  16 step     3400 |     70 batches | lr 0.000327 | ms/batch 472.18 | teacher_loss  4.03 | student_loss  3.36 | teacher ppl    56.441 | student ppl  28.91674
| epoch  17 step     3600 |     48 batches | lr 0.000275 | ms/batch 472.26 | teacher_loss  4.01 | student_loss  3.33 | teacher ppl    55.145 | student ppl  27.96214
| epoch  18 step     3800 |     26 batches | lr 0.000226 | ms/batch 472.03 | teacher_loss  3.97 | student_loss  3.29 | teacher ppl    52.995 | student ppl  26.89544
| epoch  19 step     4000 |      4 batches | lr 0.00018 | ms/batch 472.24 | teacher_loss  3.94 | student_loss  3.26 | teacher ppl    51.471 | student ppl  25.99913
----------------------------------------------------------------------------------------------------
| Eval   4 at step     4000 | time: 473.07s | teacher valid loss  4.27 | student valid loss  4.36 | teacher valid ppl    71.484 | student valid ppl    78.104
----------------------------------------------------------------------------------------------------
| epoch  19 step     4200 |    204 batches | lr 0.000139 | ms/batch 479.44 | teacher_loss  3.91 | student_loss  3.22 | teacher ppl    50.143 | student ppl  25.10844
| epoch  20 step     4400 |    182 batches | lr 0.000102 | ms/batch 472.22 | teacher_loss  3.90 | student_loss  3.20 | teacher ppl    49.294 | student ppl  24.58764
| epoch  21 step     4600 |    160 batches | lr 7.06e-05 | ms/batch 472.62 | teacher_loss  3.88 | student_loss  3.18 | teacher ppl    48.406 | student ppl  24.15095
| epoch  22 step     4800 |    138 batches | lr 4.44e-05 | ms/batch 472.32 | teacher_loss  3.87 | student_loss  3.17 | teacher ppl    48.155 | student ppl  23.90642
| epoch  23 step     5000 |    116 batches | lr 2.4e-05 | ms/batch 472.22 | teacher_loss  3.85 | student_loss  3.15 | teacher ppl    46.797 | student ppl  23.32726
----------------------------------------------------------------------------------------------------
| Eval   5 at step     5000 | time: 473.34s | teacher valid loss  4.26 | student valid loss  4.37 | teacher valid ppl    70.866 | student valid ppl    79.100
----------------------------------------------------------------------------------------------------
| epoch  24 step     5200 |     94 batches | lr 9.78e-06 | ms/batch 478.78 | teacher_loss  3.84 | student_loss  3.14 | teacher ppl    46.626 | student ppl  23.18603
| epoch  25 step     5400 |     72 batches | lr 1.8e-06 | ms/batch 472.30 | teacher_loss  3.83 | student_loss  3.13 | teacher ppl    45.957 | student ppl  22.86100
----------------------------------------------------------------------------------------------------
End of training
====================================================================================================
| End of training | teacher test loss  4.19 | teacher test ppl    66.099 | student test loss  4.30 | student test ppl    73.493
====================================================================================================
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                 lr ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   student_test_ppl ‚ñÅ
wandb: student_train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  student_valid_ppl ‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñà
wandb:   teacher_test_ppl ‚ñÅ
wandb: teacher_train_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  teacher_valid_ppl ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                 lr 0.0
wandb:   student_test_ppl 73.49327
wandb: student_train_loss 3.12943
wandb:  student_valid_ppl 79.0996
wandb:   teacher_test_ppl 66.09934
wandb: teacher_train_loss 3.82771
wandb:  teacher_valid_ppl 70.86579
wandb: 
wandb: üöÄ View run Transformer_PTB_LoT_max_epoch25_alpha0.5_N5_T1.5_lr0.001_gpu2 at: https://wandb.ai/jincan333/Transformer_LoT/runs/1dn05eke
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240202_052448-1dn05eke/logs
