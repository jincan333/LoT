Experiment dir : LM-TFM-wt103/20240202-064844
wandb: Currently logged in as: jincan333. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/caj20001/.netrc
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.11
wandb: Run data is saved locally in /home/caj20001/LoT/wandb/run-20240202_064845-4rz3vq1w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Transformer_PTB_LoT_max_epoch60_alpha0.5_N5_T1.5_lr0.001_gpu1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jincan333/Transformer_LoT
wandb: üöÄ View run at https://wandb.ai/jincan333/Transformer_LoT/runs/4rz3vq1w
Loading cached dataset...
====================================================================================================
    - data : data/ptb/
    - dataset : wt103
    - n_layer : 4
    - n_head : 8
    - d_head : 20
    - d_embed : 200
    - d_model : 200
    - d_inner : 1000
    - dropout : 0.25
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.001
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.35
    - clip_nonemb : False
    - max_step : 13320
    - batch_size : 60
    - batch_chunk : 1
    - tgt_len : 70
    - eval_tgt_len : 70
    - ext_len : 0
    - mem_len : 70
    - not_tied : False
    - seed : 1
    - cuda : True
    - gpu : 1
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : False
    - log_interval : 200
    - eval_interval : 1000
    - work_dir : LM-TFM-wt103/20240202-064844
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : -1
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - alpha : 0.5
    - student_steps_ratio : 5
    - T : 1.5
    - exp_name : Transformer_PTB_LoT_max_epoch60_alpha0.5_N5_T1.5_lr0.001_gpu1
    - max_epoch : 60
    - start_epoch : -1
    - auto_step : 1
    - tied : True
    - n_token : 10000
    - n_all_param : 8516640
    - n_nonemb_param : 4496000
====================================================================================================
#params = 8516640
#non emb params = 4496000
/home/caj20001/miniconda3/envs/LoT/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
| epoch   1 step      200 |    200 batches | lr 0.000999 | ms/batch 296.67 | teacher_loss  6.45 | student_loss  5.49 | teacher ppl   634.728 | student ppl 242.72688
| epoch   2 step      400 |    178 batches | lr 0.000998 | ms/batch 297.09 | teacher_loss  5.58 | student_loss  4.85 | teacher ppl   264.908 | student ppl 127.55808
| epoch   3 step      600 |    156 batches | lr 0.000995 | ms/batch 295.47 | teacher_loss  5.31 | student_loss  4.67 | teacher ppl   201.446 | student ppl 106.27712
| epoch   4 step      800 |    134 batches | lr 0.000991 | ms/batch 297.39 | teacher_loss  5.15 | student_loss  4.55 | teacher ppl   171.872 | student ppl  94.96392
| epoch   5 step     1000 |    112 batches | lr 0.000986 | ms/batch 298.11 | teacher_loss  5.03 | student_loss  4.47 | teacher ppl   152.394 | student ppl  87.41792
----------------------------------------------------------------------------------------------------
| Eval   1 at step     1000 | time: 297.86s | teacher valid loss  4.69 | student valid loss  4.41 | teacher valid ppl   108.825 | student valid ppl    82.579
----------------------------------------------------------------------------------------------------
| epoch   6 step     1200 |     90 batches | lr 0.00098 | ms/batch 303.27 | teacher_loss  4.94 | student_loss  4.43 | teacher ppl   140.311 | student ppl  83.75801
| epoch   7 step     1400 |     68 batches | lr 0.000973 | ms/batch 296.68 | teacher_loss  4.87 | student_loss  4.38 | teacher ppl   129.992 | student ppl  79.98954
| epoch   8 step     1600 |     46 batches | lr 0.000965 | ms/batch 297.19 | teacher_loss  4.82 | student_loss  4.35 | teacher ppl   124.125 | student ppl  77.82351
