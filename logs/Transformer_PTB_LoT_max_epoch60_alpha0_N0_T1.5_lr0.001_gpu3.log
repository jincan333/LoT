Experiment dir : LM-TFM-wt103/20240202-052821
wandb: Currently logged in as: jincan333. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/caj20001/.netrc
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.11
wandb: Run data is saved locally in /home/caj20001/LoT/wandb/run-20240202_052823-rx121klz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Transformer_PTB_LoT_max_epoch60_alpha0_N0_T1.5_lr0.001_gpu3
wandb: â­ï¸ View project at https://wandb.ai/jincan333/Transformer_LoT
wandb: ğŸš€ View run at https://wandb.ai/jincan333/Transformer_LoT/runs/rx121klz
Loading cached dataset...
====================================================================================================
    - data : data/ptb/
    - dataset : wt103
    - n_layer : 4
    - n_head : 8
    - d_head : 20
    - d_embed : 200
    - d_model : 200
    - d_inner : 1000
    - dropout : 0.25
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.001
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.35
    - clip_nonemb : False
    - max_step : 13320
    - batch_size : 60
    - batch_chunk : 1
    - tgt_len : 70
    - eval_tgt_len : 70
    - ext_len : 0
    - mem_len : 70
    - not_tied : False
    - seed : 1
    - cuda : True
    - gpu : 3
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : False
    - log_interval : 200
    - eval_interval : 1000
    - work_dir : LM-TFM-wt103/20240202-052821
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : -1
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - alpha : 0.0
    - student_steps_ratio : 0
    - T : 1.5
    - exp_name : Transformer_PTB_LoT_max_epoch60_alpha0_N0_T1.5_lr0.001_gpu3
    - max_epoch : 60
    - start_epoch : -1
    - auto_step : 1
    - tied : True
    - n_token : 10000
    - n_all_param : 8516640
    - n_nonemb_param : 4496000
====================================================================================================
#params = 8516640
#non emb params = 4496000
/home/caj20001/miniconda3/envs/LoT/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
| epoch   1 step      200 |    200 batches | lr 0.000999 | ms/batch 157.32 | teacher_loss  6.25 | student_loss  6.17 | teacher ppl   518.619 | student ppl 478.77930
| epoch   2 step      400 |    178 batches | lr 0.000998 | ms/batch 148.03 | teacher_loss  5.38 | student_loss  5.34 | teacher ppl   217.417 | student ppl 208.73004
| epoch   3 step      600 |    156 batches | lr 0.000995 | ms/batch 154.87 | teacher_loss  5.08 | student_loss  5.06 | teacher ppl   161.250 | student ppl 157.18577
| epoch   4 step      800 |    134 batches | lr 0.000991 | ms/batch 154.78 | teacher_loss  4.90 | student_loss  4.88 | teacher ppl   134.162 | student ppl 132.14133
| epoch   5 step     1000 |    112 batches | lr 0.000986 | ms/batch 153.68 | teacher_loss  4.76 | student_loss  4.75 | teacher ppl   116.391 | student ppl 115.04352
----------------------------------------------------------------------------------------------------
| Eval   1 at step     1000 | time: 156.43s | teacher valid loss  4.75 | student valid loss  4.75 | teacher valid ppl   115.550 | student valid ppl   115.154
----------------------------------------------------------------------------------------------------
| epoch   6 step     1200 |     90 batches | lr 0.00098 | ms/batch 168.37 | teacher_loss  4.65 | student_loss  4.65 | teacher ppl   105.094 | student ppl 104.57832
| epoch   7 step     1400 |     68 batches | lr 0.000973 | ms/batch 91.94 | teacher_loss  4.57 | student_loss  4.56 | teacher ppl    96.174 | student ppl  95.67674
| epoch   8 step     1600 |     46 batches | lr 0.000965 | ms/batch 71.51 | teacher_loss  4.51 | student_loss  4.50 | teacher ppl    90.778 | student ppl  90.44208
| epoch   9 step     1800 |     24 batches | lr 0.000956 | ms/batch 71.52 | teacher_loss  4.44 | student_loss  4.44 | teacher ppl    85.055 | student ppl  85.04634
| epoch  10 step     2000 |      2 batches | lr 0.000945 | ms/batch 71.71 | teacher_loss  4.39 | student_loss  4.39 | teacher ppl    80.740 | student ppl  80.90087
----------------------------------------------------------------------------------------------------
| Eval   2 at step     2000 | time: 92.97s | teacher valid loss  4.57 | student valid loss  4.57 | teacher valid ppl    96.187 | student valid ppl    96.822
----------------------------------------------------------------------------------------------------
| epoch  10 step     2200 |    202 batches | lr 0.000934 | ms/batch 76.58 | teacher_loss  4.35 | student_loss  4.35 | teacher ppl    77.226 | student ppl  77.36114
| epoch  11 step     2400 |    180 batches | lr 0.000922 | ms/batch 71.62 | teacher_loss  4.31 | student_loss  4.31 | teacher ppl    74.615 | student ppl  74.80115
| epoch  12 step     2600 |    158 batches | lr 0.000909 | ms/batch 71.63 | teacher_loss  4.28 | student_loss  4.28 | teacher ppl    72.116 | student ppl  72.40798
| epoch  13 step     2800 |    136 batches | lr 0.000895 | ms/batch 71.66 | teacher_loss  4.25 | student_loss  4.26 | teacher ppl    70.213 | student ppl  70.46556
| epoch  14 step     3000 |    114 batches | lr 0.00088 | ms/batch 71.64 | teacher_loss  4.21 | student_loss  4.21 | teacher ppl    67.261 | student ppl  67.55416
----------------------------------------------------------------------------------------------------
| Eval   3 at step     3000 | time: 72.52s | teacher valid loss  4.51 | student valid loss  4.51 | teacher valid ppl    90.523 | student valid ppl    90.971
----------------------------------------------------------------------------------------------------
| epoch  15 step     3200 |     92 batches | lr 0.000864 | ms/batch 76.33 | teacher_loss  4.18 | student_loss  4.19 | teacher ppl    65.484 | student ppl  65.86764
| epoch  16 step     3400 |     70 batches | lr 0.000848 | ms/batch 71.65 | teacher_loss  4.15 | student_loss  4.15 | teacher ppl    63.570 | student ppl  63.71377
| epoch  17 step     3600 |     48 batches | lr 0.00083 | ms/batch 71.79 | teacher_loss  4.14 | student_loss  4.14 | teacher ppl    62.609 | student ppl  62.81824
| epoch  18 step     3800 |     26 batches | lr 0.000812 | ms/batch 71.68 | teacher_loss  4.11 | student_loss  4.11 | teacher ppl    60.996 | student ppl  61.11939
| epoch  19 step     4000 |      4 batches | lr 0.000794 | ms/batch 71.68 | teacher_loss  4.09 | student_loss  4.09 | teacher ppl    59.692 | student ppl  59.79181
----------------------------------------------------------------------------------------------------
| Eval   4 at step     4000 | time: 72.43s | teacher valid loss  4.48 | student valid loss  4.48 | teacher valid ppl    88.145 | student valid ppl    87.851
----------------------------------------------------------------------------------------------------
| epoch  19 step     4200 |    204 batches | lr 0.000774 | ms/batch 76.34 | teacher_loss  4.06 | student_loss  4.07 | teacher ppl    58.209 | student ppl  58.46897
| epoch  20 step     4400 |    182 batches | lr 0.000754 | ms/batch 71.72 | teacher_loss  4.05 | student_loss  4.05 | teacher ppl    57.634 | student ppl  57.66311
| epoch  21 step     4600 |    160 batches | lr 0.000733 | ms/batch 71.69 | teacher_loss  4.04 | student_loss  4.04 | teacher ppl    56.662 | student ppl  56.66519
| epoch  22 step     4800 |    138 batches | lr 0.000712 | ms/batch 71.64 | teacher_loss  4.03 | student_loss  4.03 | teacher ppl    56.292 | student ppl  56.40115
| epoch  23 step     5000 |    116 batches | lr 0.000691 | ms/batch 71.65 | teacher_loss  4.00 | student_loss  4.00 | teacher ppl    54.355 | student ppl  54.56759
----------------------------------------------------------------------------------------------------
| Eval   5 at step     5000 | time: 72.54s | teacher valid loss  4.45 | student valid loss  4.44 | teacher valid ppl    85.919 | student valid ppl    84.816
----------------------------------------------------------------------------------------------------
| epoch  24 step     5200 |     94 batches | lr 0.000669 | ms/batch 76.45 | teacher_loss  3.98 | student_loss  3.98 | teacher ppl    53.737 | student ppl  53.78368
| epoch  25 step     5400 |     72 batches | lr 0.000646 | ms/batch 71.53 | teacher_loss  3.96 | student_loss  3.96 | teacher ppl    52.487 | student ppl  52.57712
| epoch  26 step     5600 |     50 batches | lr 0.000624 | ms/batch 71.44 | teacher_loss  3.96 | student_loss  3.96 | teacher ppl    52.297 | student ppl  52.26745
| epoch  27 step     5800 |     28 batches | lr 0.000601 | ms/batch 71.77 | teacher_loss  3.94 | student_loss  3.94 | teacher ppl    51.192 | student ppl  51.28818
| epoch  28 step     6000 |      6 batches | lr 0.000578 | ms/batch 71.48 | teacher_loss  3.92 | student_loss  3.92 | teacher ppl    50.480 | student ppl  50.54615
----------------------------------------------------------------------------------------------------
| Eval   6 at step     6000 | time: 72.40s | teacher valid loss  4.45 | student valid loss  4.44 | teacher valid ppl    85.611 | student valid ppl    85.127
----------------------------------------------------------------------------------------------------
| epoch  28 step     6200 |    206 batches | lr 0.000554 | ms/batch 76.44 | teacher_loss  3.91 | student_loss  3.91 | teacher ppl    49.687 | student ppl  49.67727
| epoch  29 step     6400 |    184 batches | lr 0.000531 | ms/batch 71.36 | teacher_loss  3.90 | student_loss  3.90 | teacher ppl    49.217 | student ppl  49.29922
| epoch  30 step     6600 |    162 batches | lr 0.000507 | ms/batch 71.41 | teacher_loss  3.88 | student_loss  3.89 | teacher ppl    48.493 | student ppl  48.69782
| epoch  31 step     6800 |    140 batches | lr 0.000483 | ms/batch 71.36 | teacher_loss  3.89 | student_loss  3.89 | teacher ppl    48.759 | student ppl  48.85450
| epoch  32 step     7000 |    118 batches | lr 0.00046 | ms/batch 71.25 | teacher_loss  3.86 | student_loss  3.86 | teacher ppl    47.285 | student ppl  47.36866
----------------------------------------------------------------------------------------------------
| Eval   7 at step     7000 | time: 72.22s | teacher valid loss  4.43 | student valid loss  4.42 | teacher valid ppl    83.647 | student valid ppl    83.393
----------------------------------------------------------------------------------------------------
| epoch  33 step     7200 |     96 batches | lr 0.000436 | ms/batch 76.14 | teacher_loss  3.85 | student_loss  3.85 | teacher ppl    46.875 | student ppl  46.97219
| epoch  34 step     7400 |     74 batches | lr 0.000413 | ms/batch 71.41 | teacher_loss  3.83 | student_loss  3.83 | teacher ppl    46.112 | student ppl  46.15364
| epoch  35 step     7600 |     52 batches | lr 0.00039 | ms/batch 71.35 | teacher_loss  3.83 | student_loss  3.83 | teacher ppl    45.930 | student ppl  45.96621
| epoch  36 step     7800 |     30 batches | lr 0.000367 | ms/batch 71.37 | teacher_loss  3.82 | student_loss  3.82 | teacher ppl    45.532 | student ppl  45.45206
| epoch  37 step     8000 |      8 batches | lr 0.000345 | ms/batch 71.24 | teacher_loss  3.80 | student_loss  3.80 | teacher ppl    44.700 | student ppl  44.87521
----------------------------------------------------------------------------------------------------
| Eval   8 at step     8000 | time: 72.11s | teacher valid loss  4.43 | student valid loss  4.43 | teacher valid ppl    84.315 | student valid ppl    83.942
----------------------------------------------------------------------------------------------------
| epoch  37 step     8200 |    208 batches | lr 0.000322 | ms/batch 75.40 | teacher_loss  3.79 | student_loss  3.79 | teacher ppl    44.323 | student ppl  44.42002
| epoch  38 step     8400 |    186 batches | lr 0.000301 | ms/batch 71.25 | teacher_loss  3.78 | student_loss  3.79 | teacher ppl    44.010 | student ppl  44.12270
| epoch  39 step     8600 |    164 batches | lr 0.000279 | ms/batch 71.18 | teacher_loss  3.77 | student_loss  3.78 | teacher ppl    43.582 | student ppl  43.61030
| epoch  40 step     8800 |    142 batches | lr 0.000258 | ms/batch 71.22 | teacher_loss  3.78 | student_loss  3.79 | teacher ppl    44.034 | student ppl  44.16543
| epoch  41 step     9000 |    120 batches | lr 0.000238 | ms/batch 71.36 | teacher_loss  3.76 | student_loss  3.76 | teacher ppl    42.768 | student ppl  42.88476
----------------------------------------------------------------------------------------------------
| Eval   9 at step     9000 | time: 72.09s | teacher valid loss  4.42 | student valid loss  4.42 | teacher valid ppl    83.060 | student valid ppl    83.002
----------------------------------------------------------------------------------------------------
| epoch  42 step     9200 |     98 batches | lr 0.000218 | ms/batch 75.96 | teacher_loss  3.75 | student_loss  3.75 | teacher ppl    42.560 | student ppl  42.70734
| epoch  43 step     9400 |     76 batches | lr 0.000199 | ms/batch 71.42 | teacher_loss  3.74 | student_loss  3.74 | teacher ppl    42.017 | student ppl  42.13689
| epoch  44 step     9600 |     54 batches | lr 0.00018 | ms/batch 71.35 | teacher_loss  3.73 | student_loss  3.74 | teacher ppl    41.887 | student ppl  42.09563
| epoch  45 step     9800 |     32 batches | lr 0.000163 | ms/batch 71.38 | teacher_loss  3.73 | student_loss  3.73 | teacher ppl    41.808 | student ppl  41.85219
| epoch  46 step    10000 |     10 batches | lr 0.000146 | ms/batch 71.34 | teacher_loss  3.72 | student_loss  3.72 | teacher ppl    41.206 | student ppl  41.31705
----------------------------------------------------------------------------------------------------
| Eval  10 at step    10000 | time: 72.22s | teacher valid loss  4.43 | student valid loss  4.42 | teacher valid ppl    83.884 | student valid ppl    83.024
----------------------------------------------------------------------------------------------------
| epoch  46 step    10200 |    210 batches | lr 0.000129 | ms/batch 75.73 | teacher_loss  3.71 | student_loss  3.72 | teacher ppl    41.054 | student ppl  41.10134
| epoch  47 step    10400 |    188 batches | lr 0.000114 | ms/batch 71.37 | teacher_loss  3.71 | student_loss  3.71 | teacher ppl    40.853 | student ppl  40.95976
| epoch  48 step    10600 |    166 batches | lr 9.94e-05 | ms/batch 71.41 | teacher_loss  3.71 | student_loss  3.71 | teacher ppl    40.749 | student ppl  40.67864
| epoch  49 step    10800 |    144 batches | lr 8.57e-05 | ms/batch 71.32 | teacher_loss  3.72 | student_loss  3.72 | teacher ppl    41.328 | student ppl  41.32822
| epoch  50 step    11000 |    122 batches | lr 7.3e-05 | ms/batch 71.37 | teacher_loss  3.70 | student_loss  3.70 | teacher ppl    40.293 | student ppl  40.35039
----------------------------------------------------------------------------------------------------
| Eval  11 at step    11000 | time: 72.16s | teacher valid loss  4.42 | student valid loss  4.42 | teacher valid ppl    83.147 | student valid ppl    82.967
----------------------------------------------------------------------------------------------------
| epoch  51 step    11200 |    100 batches | lr 6.12e-05 | ms/batch 75.17 | teacher_loss  3.69 | student_loss  3.70 | teacher ppl    40.232 | student ppl  40.34172
| epoch  52 step    11400 |     78 batches | lr 5.04e-05 | ms/batch 71.39 | teacher_loss  3.69 | student_loss  3.69 | teacher ppl    39.988 | student ppl  39.94938
| epoch  53 step    11600 |     56 batches | lr 4.06e-05 | ms/batch 71.40 | teacher_loss  3.68 | student_loss  3.69 | teacher ppl    39.776 | student ppl  39.85719
| epoch  54 step    11800 |     34 batches | lr 3.18e-05 | ms/batch 71.35 | teacher_loss  3.69 | student_loss  3.69 | teacher ppl    39.944 | student ppl  40.04112
| epoch  55 step    12000 |     12 batches | lr 2.4e-05 | ms/batch 71.49 | teacher_loss  3.68 | student_loss  3.68 | teacher ppl    39.455 | student ppl  39.54103
----------------------------------------------------------------------------------------------------
| Eval  12 at step    12000 | time: 72.24s | teacher valid loss  4.42 | student valid loss  4.42 | teacher valid ppl    83.403 | student valid ppl    83.235
----------------------------------------------------------------------------------------------------
| epoch  55 step    12200 |    212 batches | lr 1.73e-05 | ms/batch 75.89 | teacher_loss  3.68 | student_loss  3.68 | teacher ppl    39.591 | student ppl  39.56493
| epoch  56 step    12400 |    190 batches | lr 1.17e-05 | ms/batch 71.23 | teacher_loss  3.67 | student_loss  3.68 | teacher ppl    39.414 | student ppl  39.48114
| epoch  57 step    12600 |    168 batches | lr 7.19e-06 | ms/batch 71.24 | teacher_loss  3.68 | student_loss  3.68 | teacher ppl    39.479 | student ppl  39.56710
| epoch  58 step    12800 |    146 batches | lr 3.76e-06 | ms/batch 71.26 | teacher_loss  3.69 | student_loss  3.70 | teacher ppl    40.114 | student ppl  40.24760
| epoch  59 step    13000 |    124 batches | lr 1.42e-06 | ms/batch 71.57 | teacher_loss  3.67 | student_loss  3.68 | teacher ppl    39.418 | student ppl  39.49266
----------------------------------------------------------------------------------------------------
| Eval  13 at step    13000 | time: 72.17s | teacher valid loss  4.42 | student valid loss  4.42 | teacher valid ppl    83.488 | student valid ppl    83.221
----------------------------------------------------------------------------------------------------
| epoch  60 step    13200 |    102 batches | lr 2e-07 | ms/batch 75.15 | teacher_loss  3.67 | student_loss  3.68 | teacher ppl    39.346 | student ppl  39.46372
----------------------------------------------------------------------------------------------------
End of training
====================================================================================================
| End of training | teacher test loss  4.32 | teacher test ppl    75.341 | student test loss  4.33 | student test ppl    76.049
====================================================================================================
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                 lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:   student_test_ppl â–
wandb: student_train_loss â–ˆâ–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  student_valid_ppl â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–
wandb:   teacher_test_ppl â–
wandb: teacher_train_loss â–ˆâ–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  teacher_valid_ppl â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                 lr 0.0
wandb:   student_test_ppl 76.04891
wandb: student_train_loss 3.67538
wandb:  student_valid_ppl 83.22125
wandb:   teacher_test_ppl 75.34147
wandb: teacher_train_loss 3.6724
wandb:  teacher_valid_ppl 83.48808
wandb: 
wandb: ğŸš€ View run Transformer_PTB_LoT_max_epoch60_alpha0_N0_T1.5_lr0.001_gpu3 at: https://wandb.ai/jincan333/Transformer_LoT/runs/rx121klz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240202_052823-rx121klz/logs
