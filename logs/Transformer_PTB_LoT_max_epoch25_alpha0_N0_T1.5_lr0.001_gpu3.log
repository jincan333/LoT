Experiment dir : LM-TFM-wt103/20240202-051147
wandb: Currently logged in as: jincan333. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/caj20001/.netrc
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.11
wandb: Run data is saved locally in /home/caj20001/LoT/wandb/run-20240202_051148-enqr4t0k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Transformer_PTB_LoT_max_epoch25_alpha0_N0_T1.5_lr0.001_gpu3
wandb: â­ï¸ View project at https://wandb.ai/jincan333/Transformer_LoT
wandb: ğŸš€ View run at https://wandb.ai/jincan333/Transformer_LoT/runs/enqr4t0k
Loading cached dataset...
====================================================================================================
    - data : data/ptb/
    - dataset : wt103
    - n_layer : 4
    - n_head : 12
    - d_head : 41
    - d_embed : 410
    - d_model : 410
    - d_inner : 2100
    - dropout : 0.25
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.001
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.35
    - clip_nonemb : False
    - max_step : 5550
    - batch_size : 60
    - batch_chunk : 1
    - tgt_len : 70
    - eval_tgt_len : 70
    - ext_len : 0
    - mem_len : 70
    - not_tied : False
    - seed : 1
    - cuda : True
    - gpu : 3
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : False
    - log_interval : 200
    - eval_interval : 1000
    - work_dir : LM-TFM-wt103/20240202-051147
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : -1
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - alpha : 0.0
    - student_steps_ratio : 0
    - T : 1.5
    - exp_name : Transformer_PTB_LoT_max_epoch25_alpha0_N0_T1.5_lr0.001_gpu3
    - max_epoch : 25
    - start_epoch : -1
    - auto_step : 1
    - tied : True
    - n_token : 10000
    - n_all_param : 30099968
    - n_nonemb_param : 21878000
====================================================================================================
#params = 30099968
#non emb params = 21878000
/home/caj20001/miniconda3/envs/LoT/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
| epoch   1 step      200 |    200 batches | lr 0.000997 | ms/batch 285.32 | teacher_loss  5.87 | student_loss  5.89 | teacher ppl   355.981 | student ppl 360.99740
| epoch   2 step      400 |    178 batches | lr 0.000987 | ms/batch 282.34 | teacher_loss  5.05 | student_loss  5.06 | teacher ppl   156.461 | student ppl 158.14712
| epoch   3 step      600 |    156 batches | lr 0.000971 | ms/batch 272.92 | teacher_loss  4.75 | student_loss  4.76 | teacher ppl   115.129 | student ppl 116.49713
| epoch   4 step      800 |    134 batches | lr 0.00095 | ms/batch 282.24 | teacher_loss  4.56 | student_loss  4.56 | teacher ppl    95.146 | student ppl  96.05156
| epoch   5 step     1000 |    112 batches | lr 0.000922 | ms/batch 282.22 | teacher_loss  4.41 | student_loss  4.41 | teacher ppl    81.894 | student ppl  82.31352
----------------------------------------------------------------------------------------------------
| Eval   1 at step     1000 | time: 284.12s | teacher valid loss  4.60 | student valid loss  4.60 | teacher valid ppl    99.402 | student valid ppl    99.608
----------------------------------------------------------------------------------------------------
| epoch   6 step     1200 |     90 batches | lr 0.000889 | ms/batch 299.21 | teacher_loss  4.29 | student_loss  4.30 | teacher ppl    73.109 | student ppl  73.34873
| epoch   7 step     1400 |     68 batches | lr 0.000851 | ms/batch 282.29 | teacher_loss  4.19 | student_loss  4.19 | teacher ppl    65.771 | student ppl  65.98406
| epoch   8 step     1600 |     46 batches | lr 0.000809 | ms/batch 272.34 | teacher_loss  4.11 | student_loss  4.11 | teacher ppl    61.001 | student ppl  61.10120
| epoch   9 step     1800 |     24 batches | lr 0.000762 | ms/batch 282.40 | teacher_loss  4.03 | student_loss  4.03 | teacher ppl    56.093 | student ppl  55.99904
| epoch  10 step     2000 |      2 batches | lr 0.000712 | ms/batch 282.25 | teacher_loss  3.95 | student_loss  3.95 | teacher ppl    52.140 | student ppl  52.08024
----------------------------------------------------------------------------------------------------
| Eval   2 at step     2000 | time: 283.35s | teacher valid loss  4.48 | student valid loss  4.50 | teacher valid ppl    88.495 | student valid ppl    89.609
----------------------------------------------------------------------------------------------------
| epoch  10 step     2200 |    202 batches | lr 0.00066 | ms/batch 300.96 | teacher_loss  3.89 | student_loss  3.89 | teacher ppl    48.777 | student ppl  48.67375
| epoch  11 step     2400 |    180 batches | lr 0.000605 | ms/batch 282.36 | teacher_loss  3.82 | student_loss  3.83 | teacher ppl    45.803 | student ppl  45.87710
| epoch  12 step     2600 |    158 batches | lr 0.000549 | ms/batch 272.36 | teacher_loss  3.77 | student_loss  3.77 | teacher ppl    43.287 | student ppl  43.33777
| epoch  13 step     2800 |    136 batches | lr 0.000493 | ms/batch 282.63 | teacher_loss  3.72 | student_loss  3.71 | teacher ppl    41.103 | student ppl  41.04055
| epoch  14 step     3000 |    114 batches | lr 0.000436 | ms/batch 282.34 | teacher_loss  3.65 | student_loss  3.65 | teacher ppl    38.384 | student ppl  38.41664
----------------------------------------------------------------------------------------------------
| Eval   3 at step     3000 | time: 283.81s | teacher valid loss  4.44 | student valid loss  4.45 | teacher valid ppl    84.700 | student valid ppl    85.446
----------------------------------------------------------------------------------------------------
| epoch  15 step     3200 |     92 batches | lr 0.000381 | ms/batch 253.84 | teacher_loss  3.60 | student_loss  3.60 | teacher ppl    36.517 | student ppl  36.50836
| epoch  16 step     3400 |     70 batches | lr 0.000327 | ms/batch 130.91 | teacher_loss  3.54 | student_loss  3.54 | teacher ppl    34.362 | student ppl  34.38727
| epoch  17 step     3600 |     48 batches | lr 0.000275 | ms/batch 130.96 | teacher_loss  3.50 | student_loss  3.50 | teacher ppl    33.210 | student ppl  33.17022
| epoch  18 step     3800 |     26 batches | lr 0.000226 | ms/batch 130.95 | teacher_loss  3.45 | student_loss  3.45 | teacher ppl    31.540 | student ppl  31.54420
| epoch  19 step     4000 |      4 batches | lr 0.00018 | ms/batch 130.95 | teacher_loss  3.41 | student_loss  3.41 | teacher ppl    30.366 | student ppl  30.38536
----------------------------------------------------------------------------------------------------
| Eval   4 at step     4000 | time: 152.84s | teacher valid loss  4.45 | student valid loss  4.45 | teacher valid ppl    85.712 | student valid ppl    85.389
----------------------------------------------------------------------------------------------------
| epoch  19 step     4200 |    204 batches | lr 0.000139 | ms/batch 135.83 | teacher_loss  3.38 | student_loss  3.38 | teacher ppl    29.390 | student ppl  29.49752
| epoch  20 step     4400 |    182 batches | lr 0.000102 | ms/batch 131.01 | teacher_loss  3.35 | student_loss  3.36 | teacher ppl    28.641 | student ppl  28.74507
| epoch  21 step     4600 |    160 batches | lr 7.06e-05 | ms/batch 131.02 | teacher_loss  3.33 | student_loss  3.33 | teacher ppl    28.057 | student ppl  28.05191
| epoch  22 step     4800 |    138 batches | lr 4.44e-05 | ms/batch 130.91 | teacher_loss  3.33 | student_loss  3.33 | teacher ppl    27.864 | student ppl  27.84085
| epoch  23 step     5000 |    116 batches | lr 2.4e-05 | ms/batch 238.06 | teacher_loss  3.30 | student_loss  3.30 | teacher ppl    26.993 | student ppl  27.05103
----------------------------------------------------------------------------------------------------
| Eval   5 at step     5000 | time: 155.38s | teacher valid loss  4.45 | student valid loss  4.45 | teacher valid ppl    85.440 | student valid ppl    85.862
----------------------------------------------------------------------------------------------------
| epoch  24 step     5200 |     94 batches | lr 9.78e-06 | ms/batch 287.55 | teacher_loss  3.29 | student_loss  3.29 | teacher ppl    26.819 | student ppl  26.93444
| epoch  25 step     5400 |     72 batches | lr 1.8e-06 | ms/batch 274.04 | teacher_loss  3.27 | student_loss  3.27 | teacher ppl    26.420 | student ppl  26.42899
----------------------------------------------------------------------------------------------------
End of training
====================================================================================================
| End of training | teacher test loss  4.37 | teacher test ppl    79.267 | student test loss  4.38 | student test ppl    79.777
====================================================================================================
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                 lr â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–
wandb:   student_test_ppl â–
wandb: student_train_loss â–ˆâ–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:  student_valid_ppl â–ˆâ–ƒâ–â–â–
wandb:   teacher_test_ppl â–
wandb: teacher_train_loss â–ˆâ–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:  teacher_valid_ppl â–ˆâ–ƒâ–â–â–
wandb: 
wandb: Run summary:
wandb:                 lr 0.0
wandb:   student_test_ppl 79.77731
wandb: student_train_loss 3.27446
wandb:  student_valid_ppl 85.86231
wandb:   teacher_test_ppl 79.26736
wandb: teacher_train_loss 3.27413
wandb:  teacher_valid_ppl 85.44033
wandb: 
wandb: ğŸš€ View run Transformer_PTB_LoT_max_epoch25_alpha0_N0_T1.5_lr0.001_gpu3 at: https://wandb.ai/jincan333/Transformer_LoT/runs/kuyqetqn
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240202_051050-kuyqetqn/logs
